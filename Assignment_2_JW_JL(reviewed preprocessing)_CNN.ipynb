{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 - Machine Learning and Data Mining: Assignment 2\n",
    "<div style=\"text-align: right\"> Group 86 </div>\n",
    "<div style=\"text-align: right\"> tlin4302 | 470322974 | Jenny Tsai-chen Lin </div>\n",
    "<div style=\"text-align: right\"> jsun4242 | 500409987 | Jiawei Sun </div>\n",
    "<div style=\"text-align: right\"> Code | ID | Name </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook includes sections :\n",
    "\n",
    "    Section 1. Library and general functions \n",
    "    Section 2. Data pre-processing\n",
    "    Section 3. Implement algorithms\n",
    "        3.1 AdaBoost Classifier\n",
    "        3.2 Support-Vector-Machine Classifier\n",
    "        3.3 Convolutional Neural Network Classifier\n",
    "    Section 4. Compare result between algorithms in train dataset \n",
    "    Section 5: Best perfroming algorithms in testing data  (we will submit this in seperate notebook as well)\n",
    "    Section 6. Hardware and software specifications\n",
    "\n",
    "In response to 4 main parts of report requirement: \n",
    "\n",
    "    1. Try 3 different Machine Learning methods and compare their performance. | Section 3 & Section 4\n",
    "    2. Choosing an appropriate model and its complexity | Section 3\n",
    "    3. Using pre-processing techniques on the datasets | Section 2\n",
    "    4. Computer infrastructure   | TBC\n",
    "    5. Ease of prototyping\n",
    "    6. Hardware and software specifications of the computer that you used for performance evaluation | Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_CNN_train = False\n",
    "\n",
    "test_CNN_predict = True\n",
    "\n",
    "test_CNN_ten_fold = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Library and general functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Go to anaconda prompt to install package imblearn\n",
    "# anaconda: conda install -c glemaitre imbalanced-learn\n",
    "#pip install kmeans-smote\n",
    "\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one of below two line depend file location******  [JL_UPDATE description]\n",
    "\n",
    "g_dataset_dir = \"./dataset/\"   \n",
    "#g_dataset_dir = \"../dataset/\" \n",
    "\n",
    "\n",
    "a_random_file = \"./dataset/train/1b1B1b2-2pK2q1-4p1rB-7k-8-8-3B4-3rb3.jpeg\"  \n",
    "#a_random_file = \"../dataset/train/1b1B1b2-2pK2q1-4p1rB-7k-8-8-3B4-3rb3.jpeg\" \n",
    "\n",
    "saved_model_path = \"./saved_model/\"\n",
    "abc_model_file = saved_model_path + \"abc_dump.pkl\"\n",
    "svc_model_file = saved_model_path + \"svc_dump.pkl\"\n",
    "cnn_model_file = saved_model_path + \"cnn_weights\"\n",
    "\n",
    "ten_fold_result_path = \"./ten_fold_results/\"\n",
    "\n",
    "\n",
    "# define global variable \n",
    "\n",
    "g_train_dir = g_dataset_dir + \"/train/\"\n",
    "g_test_dir = g_dataset_dir + \"/test/\"\n",
    "\n",
    "g_image_size = 400\n",
    "\n",
    "g_grid_row = 8\n",
    "g_grid_col = 8\n",
    "\n",
    "g_grid_num = g_grid_row * g_grid_col\n",
    "g_grid_size = int(g_image_size / g_grid_row)\n",
    "\n",
    "\n",
    "#Processing 1 - scale down \n",
    "g_down_sampled_size = 200\n",
    "g_down_sampled_grid_size = int(g_grid_size / (g_image_size / g_down_sampled_size))\n",
    "\n",
    "# global instance of mapping of char vs chess pieces\n",
    "# reference: Forsythâ€“Edwards Notation, https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation\n",
    "# \n",
    "# pawn = \"P\", knight = \"N\", bishop = \"B\", rook = \"R\", queen = \"Q\" and king = \"K\"\n",
    "# White pieces are designated using upper-case letters (\"PNBRQK\") while black pieces use lowercase (\"pnbrqk\")\n",
    "# we use 0 to note an empty grid.\n",
    "# 13 items in total.\n",
    "\n",
    "g_piece_mapping = {\n",
    "    \"P\" : \"pawn\",\n",
    "    \"N\" : \"knight\",\n",
    "    \"B\" : \"bishop\",\n",
    "    \"R\" : \"rook\",\n",
    "    \"Q\" : \"queen\",\n",
    "    \"K\" : \"king\",\n",
    "\n",
    "    \"p\" : \"pawn\",\n",
    "    \"n\" : \"knight\",\n",
    "    \"b\" : \"bishop\",\n",
    "    \"r\" : \"rook\",\n",
    "    \"q\" : \"queen\",\n",
    "    \"k\" : \"king\",\n",
    "\n",
    "    \"0\" : \"empty_grid\"\n",
    "}\n",
    "\n",
    "g_num_labels = len(g_piece_mapping)\n",
    "\n",
    "g_labels = [\"P\",\n",
    "\"N\",\n",
    "\"B\",\n",
    "\"R\",\n",
    "\"Q\",\n",
    "\"K\",\n",
    "\"p\",\n",
    "\"n\",\n",
    "\"b\",\n",
    "\"r\",\n",
    "\"q\",\n",
    "\"k\",\n",
    "\"0\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper codes for label & board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataHelper.py\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "# get clean name by a path, where in our case this gets the FEN conviniently\n",
    "def GetCleanNameByPath(file_name):\n",
    "    return os.path.splitext(os.path.basename(file_name))[0]\n",
    "\n",
    "# get full paths to the files in a directory.\n",
    "def GetFileNamesInDir(path_name, extension=\"*\", num_return = 0):\n",
    "    if num_return == 0:\n",
    "        return glob.glob(path_name + \"/*.\" + extension)\n",
    "    else:\n",
    "        return glob.glob(path_name + \"/*.\" + extension)[:num_return]\n",
    "\n",
    "# get name list\n",
    "def GetCleanNamesInDir(path_name, extension = \"*\", num_return = 0):\n",
    "    names = GetFileNamesInDir(path_name, extension)\n",
    "    offset = len(extension) + 1\n",
    "    clean_names = [os.path.basename(x)[:-offset] for x in names]\n",
    "    if num_return == 0:\n",
    "        return clean_names\n",
    "    else:\n",
    "        return clean_names[:num_return]\n",
    "\n",
    "# read dataset\n",
    "def ReadImages(file_names, path = \"\", format = cv2.IMREAD_COLOR):\n",
    "    if path == \"\":\n",
    "        return [cv2.imread(f, format) for f in file_names]\n",
    "    else:\n",
    "        return [cv2.imread(path + \"/\" + f, format) for f in file_names]\n",
    "\n",
    "# read image by name\n",
    "def ReadImage(file_name, gray = False):\n",
    "    return io.imread(file_name, as_gray = gray)\n",
    "\n",
    "\n",
    "# h5py functions\n",
    "    \n",
    "# read h5py file\n",
    "# we assume the labels and \n",
    "def ReadH5pyFile(file_name, data_name):\n",
    "    h5_buffer = h5py.File(file_name)\n",
    "    return h5_buffer[data_name].copy()\n",
    "\n",
    "# write h5py file\n",
    "def WriteH5pyFile(file_name, mat, data_name = \"dataset\"):\n",
    "    with h5py.File(file_name, 'w') as f:\n",
    "        f.create_dataset(data_name, data = mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoardHelper.py\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import OrderedDict \n",
    "\n",
    "import numpy as np\n",
    "import skimage.util\n",
    "from skimage.util.shape import view_as_blocks\n",
    "\n",
    "#from ChessGlobalDefs import *  # [JL_not able to load]\n",
    "\n",
    "#FEN TO LABELS OF SQUARES\n",
    "def FENtoL(fen): \n",
    "    rules = {\n",
    "        r\"-\": r\"\",\n",
    "        r\"1\": r\"0\",\n",
    "        r\"2\": r\"00\",\n",
    "        r\"3\": r\"000\",\n",
    "        r\"4\": r\"0000\",\n",
    "        r\"5\": r\"00000\",\n",
    "        r\"6\": r\"000000\",\n",
    "        r\"7\": r\"0000000\",\n",
    "        r\"8\": r\"00000000\",\n",
    "    }\n",
    "\n",
    "    for key in rules.keys():\n",
    "        fen = re.sub(key, rules[key], fen)\n",
    "\n",
    "    return list(fen)\n",
    "\n",
    "\n",
    "# Label array to char list:\n",
    "def LabelArrayToL(arr):\n",
    "    rules = {\n",
    "        0 : \"P\",\n",
    "        1 : \"N\",\n",
    "        2 : \"B\",\n",
    "        3 : \"R\",\n",
    "        4 : \"Q\",\n",
    "        5 : \"K\",\n",
    "\n",
    "        6 : \"p\",\n",
    "        7 : \"n\",\n",
    "        8 : \"b\",\n",
    "        9 : \"r\",\n",
    "       10 : \"q\",\n",
    "       11 : \"k\",\n",
    "\n",
    "       12 : \"0\"\n",
    "    }\n",
    "\n",
    "    flattened = arr.flatten(order = \"C\")\n",
    "\n",
    "    L = []\n",
    "\n",
    "    for x in flattened:\n",
    "        L.append(rules[x])\n",
    "\n",
    "    return L\n",
    "\n",
    "# char list to FEN\n",
    "def LtoFEN(L):\n",
    "\n",
    "    FEN = \"\"\n",
    "    \n",
    "    for y in range(8):\n",
    "        counter = 0\n",
    "        for x in range(8):\n",
    "            idx = x + y * 8\n",
    "            char = L[idx]\n",
    "\n",
    "            if char == \"0\":\n",
    "                counter += 1\n",
    "                if x == 7:\n",
    "                    FEN += str(counter)\n",
    "            else:\n",
    "                if counter:\n",
    "                    FEN += str(counter)\n",
    "                    counter = 0\n",
    "\n",
    "                FEN += char\n",
    "        if y != 7:\n",
    "            FEN += \"-\"\n",
    "        \n",
    "            \n",
    "    return FEN\n",
    "\n",
    "\n",
    "\n",
    "# FEN to one-hot encoding, in our case, it returns an 64 by 13 array, with each row as a one-hot to a grid.\n",
    "def FENtoOneHot(fen):\n",
    "\n",
    "    # this rule is in the same format as g_piece_mapping\n",
    "    #rules = {\n",
    "    #    \"P\" : np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    #    \"N\" : np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    #    \"B\" : np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    #    \"R\" : np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    #    \"Q\" : np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    #    \"K\" : np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    #                    \n",
    "    #    \"p\" : np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
    "    #    \"n\" : np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
    "    #    \"b\" : np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
    "    #    \"r\" : np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
    "    #    \"q\" : np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
    "    #    \"k\" : np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
    "    #                    \n",
    "    #    \"0\" : np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "    #}\n",
    "    rules = {\n",
    "        \"P\" : 0,\n",
    "        \"N\" : 1,\n",
    "        \"B\" : 2,\n",
    "        \"R\" : 3,\n",
    "        \"Q\" : 4,\n",
    "        \"K\" : 5,\n",
    "\n",
    "        \"p\" : 6,\n",
    "        \"n\" : 7,\n",
    "        \"b\" : 8,\n",
    "        \"r\" : 9,\n",
    "        \"q\" : 10,\n",
    "        \"k\" : 11,\n",
    "\n",
    "        \"0\" : 12\n",
    "    }\n",
    "\n",
    "    L = FENtoL(fen)\n",
    "    one_hot_array = np.zeros((g_grid_num, g_num_labels), dtype = np.int32) # 64 by 13\n",
    "    for i, c in enumerate(L):\n",
    "        one_hot_array[i, rules[c]] = 1\n",
    "\n",
    "    return one_hot_array\n",
    "\n",
    "# get 8*8 char matrix\n",
    "def LtoCharMat(l):\n",
    "    if type(l) == list:\n",
    "        return np.array(l).reshape((8,8))\n",
    "    if type(l) == str:\n",
    "        return np.array([l]).reshape((8,8))\n",
    "\n",
    "def GetBoardCell(board_image, row = 0, col = 0, size = 50):\n",
    "    return np.array(board_image)[row*size:(row+1)*size,col*size:(col+1)*size]\n",
    "\n",
    "# get grids of image\n",
    "def ImageToGrids(image, grid_size_x, grid_size_y):\n",
    "    return skimage.util.shape.view_as_blocks(image, block_shape = (grid_size_y, grid_size_x, 3)).squeeze(axis = 2)\n",
    "\n",
    "# get grids of image\n",
    "def ImageToGrids_grey(image, grid_size_x, grid_size_y):\n",
    "    return skimage.util.shape.view_as_blocks(image, block_shape = (grid_size_y, grid_size_x, 1)).squeeze(axis = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix - heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing - generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[JL_update func_generator ]\n",
    "\n",
    "# split into 64 small square from 1 board\n",
    "# image resized to 400x 400 to 200x 200. 64 square at 25x 25 each\n",
    "\n",
    "def PreprocessImage(image):\n",
    "    image = transform.resize(image, (g_down_sampled_size, g_down_sampled_size), mode='constant')\n",
    "        \n",
    "    # 1st and 2nd dim is 8\n",
    "    grids = ImageToGrids(image, g_down_sampled_grid_size, g_down_sampled_grid_size)\n",
    "\n",
    "    return grids.reshape(g_grid_row * g_grid_col, g_down_sampled_grid_size, g_down_sampled_grid_size, 3)\n",
    "\n",
    "# split into 64 small square from 1 board -\n",
    "# output of x: number of image x 64 x 25 x 25 x 3 , y: number of image x 64 x 13\n",
    "def func_generator(train_file_names):\n",
    "    x = []\n",
    "    y = []\n",
    "    for image_file_name in train_file_names:\n",
    "        img = ReadImage(image_file_name)\n",
    "        x.append(PreprocessImage(img))\n",
    "        y.append(np.array(FENtoOneHot(GetCleanNameByPath(image_file_name))))\n",
    "        \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Implement algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base class for all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "# interface of the classifiers\n",
    "class IClassifier:\n",
    "\n",
    "    # this method should accept a list of file names of the training data\n",
    "    @abc.abstractmethod\n",
    "    def Train(self, train_file_names):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # this should accept a 400 * 400 * 3 numpy array as query data, and returns the fen notation of the board.\n",
    "    @abc.abstractmethod\n",
    "    def Predict(self, query_data):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    # this should accept a list of file names, and returns the predicted labels as 1d numpy array.\n",
    "    @abc.abstractmethod\n",
    "    def Predict(self, query_data):\n",
    "        raise NotImplementedError()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-fold related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters accepts a list of file names, and return the data matrix and labels\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# get balanced accuracy from confusion matrix\n",
    "def BalancedAccuracyFromConfusionMatrix(cm):\n",
    "    ret = np.empty((cm.shape[0]))\n",
    "    \n",
    "    for idx, row in enumerate(cm):\n",
    "        ret[idx] = row[idx] / row.sum()\n",
    "    \n",
    "    return ret.mean()\n",
    "\n",
    "# dummy filter to return all files\n",
    "def DefaultFilter(file_names, rate = 1):\n",
    "    return file_names\n",
    "\n",
    "# filter using random_sampling:\n",
    "def RandomFilter(file_names, rate = 1):\n",
    "    # we fix the random part to assure the results are consistent\n",
    "    random_seed = 4242\n",
    "    random.seed(random_seed)\n",
    "    return random.sample(file_names, k = int(len(file_names) * rate))\n",
    "    \n",
    "def ConfusionMatrix(classifier, test_file_names, filter = RandomFilter, sampling_rate = 0.001):\n",
    "    \n",
    "    confusion_matrices = []\n",
    "    accuracies = []\n",
    "    accuracies_balanced = []\n",
    "    train_time_cost = []\n",
    "    validation_time_cost = []\n",
    "    \n",
    "    # split name list into 10 equal parts\n",
    "    division = len(test_file_names) / float(10) \n",
    "    complete_name_folds = [ test_file_names[int(round(division * i)): int(round(division * (i + 1)))] for i in range(10) ]\n",
    "    filtered_name_folds = complete_name_folds.copy()\n",
    "    for i in range(10):\n",
    "        filtered_name_folds[i] = filter(complete_name_folds[i], rate = sampling_rate)\n",
    "\n",
    "    # we use filtered name folds to train, and validation.\n",
    "    for iv in range(10):\n",
    "        \n",
    "        # merge the 9 folds:\n",
    "        train_names = []\n",
    "        validation_names = []\n",
    "        for i in range(10):\n",
    "            if i != iv:\n",
    "                train_names.extend(filtered_name_folds[i])\n",
    "            else:\n",
    "                # validation_names = complete_name_folds[i].copy()\n",
    "                validation_names = filtered_name_folds[i].copy()\n",
    "\n",
    "            \n",
    "        # train the classifier:\n",
    "        print(\"training started:    \", type(classifier).__name__, \"for fold #\", iv, \"# train files:\", len(train_names))\n",
    "        t = time.time()\n",
    "        classifier.Train(train_names)\n",
    "        train_time_cost.append(time.time() - t)\n",
    "        print(\"training finished:   \", type(classifier).__name__, \"for fold #\", iv,\n",
    "              \"time: {}s\".format(time.time() - t))\n",
    "        \n",
    "        print(\"predicting started:  \", type(classifier).__name__, \"for fold #\", iv)\n",
    "        t = time.time()\n",
    "        ypreds, y_true = classifier.PredictMultiple(validation_names)\n",
    "        validation_time_cost.append(time.time() - t)\n",
    "        \n",
    "        ypreds = ypreds.reshape((-1, 1))\n",
    "        y_true = y_true.reshape((-1, 1))\n",
    "\n",
    "        conf_mat = confusion_matrix(y_true, ypreds, labels = g_labels)\n",
    "        confusion_matrices.append(conf_mat)\n",
    "        accuracy = np.trace(conf_mat) / float(np.sum(conf_mat))\n",
    "        accuracies.append(accuracy)\n",
    "        accuracy_balanced = BalancedAccuracyFromConfusionMatrix(conf_mat)\n",
    "        accuracies_balanced.append(accuracy_balanced)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"predicting finished: \", type(classifier).__name__, \"for fold #\", iv,\n",
    "              \"time: {}s\".format(time.time() - t), \" accuracy: \", accuracy, \" balanced_accuracy:\", accuracy_balanced)\n",
    "        \n",
    "    return confusion_matrices, accuracies, accuracies_balanced, train_time_cost, validation_time_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Classifier (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class definition for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import cv2\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tf.keras.models import Sequential\n",
    "#from tf.keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "#from tf.keras.layers.convolutional import Convolution2D\n",
    "\n",
    "class CNNClassifier(IClassifier):\n",
    "\n",
    "    # the file name format does not accept batch as parameter. link:\n",
    "    # https://github.com/tensorflow/tensorflow/issues/38668\n",
    "    s_check_point_file_name = \"./CNN_training_checkpoint/cp_{epoch:02d}-{accuracy:.2f}.ckpt\"\n",
    "    s_check_point_path = os.path.dirname(s_check_point_file_name)\n",
    "    s_save_frequence = 10000 # save a checkpoint every s_save_frequence batches\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #tf.config.threading.set_inter_op_parallelism_threads(3)\n",
    "        #tf.config.threading.set_intra_op_parallelism_threads(3)\n",
    "\n",
    "        # define our model\n",
    "        self.__model__ = keras.Sequential(\n",
    "            [\n",
    "                layers.Convolution2D(32, (3, 3), input_shape = (g_down_sampled_grid_size, g_down_sampled_grid_size, 3)),\n",
    "                layers.Activation('relu'),\n",
    "                layers.Dropout(0.1),\n",
    "                layers.Convolution2D(32, (3, 3)),\n",
    "                layers.Activation('relu'),\n",
    "\n",
    "                layers.Convolution2D(32, (3, 3)),\n",
    "                layers.Activation('relu'),\n",
    "\n",
    "                layers.Flatten(),\n",
    "                \n",
    "                layers.Dense(128),\n",
    "                layers.Activation('relu'),\n",
    "                layers.Dropout(0.3),\n",
    "\n",
    "                layers.Dense(13),\n",
    "                layers.Activation(\"softmax\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.__model__.compile(loss = \"categorical_crossentropy\", optimizer = 'adam', metrics = [\"accuracy\"])\n",
    "        \n",
    "        self.__save_check_point_callback__ = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = CNNClassifier.s_check_point_file_name,\n",
    "            monitor='val_accuracy',\n",
    "            save_weights_only = True,\n",
    "            save_freq = CNNClassifier.s_save_frequence,\n",
    "            verbose = 1\n",
    "            )\n",
    "\n",
    "\n",
    "     # generator\n",
    "    @staticmethod\n",
    "    def func_generator(train_file_names):\n",
    "        for image_file_name in train_file_names:\n",
    "            img = ReadImage(image_file_name)\n",
    "            x = CNNClassifier.PreprocessImage(img)\n",
    "            y = np.array(FENtoOneHot(GetCleanNameByPath(image_file_name)))\n",
    "            yield x, y\n",
    "\n",
    "    # this method should accept N * 64 * m * n numpy array as train data, and N lists of 64 chars as label.\n",
    "    def Train(self, train_data_names):\n",
    "        train_size = len(train_data_names)\n",
    "\n",
    "        ## try load last checkpoint\n",
    "        #if not self.LoadMostRecentModel():\n",
    "        #    os.makedirs(CNNClassifier.s_check_point_path, exist_ok = True)\n",
    "\n",
    "        # train\n",
    "        self.__model__.fit(CNNClassifier.func_generator(train_data_names),\n",
    "                           use_multiprocessing = False,\n",
    "                           #batch_size = 1000,\n",
    "                           steps_per_epoch = train_size / 20,\n",
    "                           epochs = 2,\n",
    "                           #callbacks = [self.__save_check_point_callback__],\n",
    "                           verbose = 1)\n",
    "\n",
    "\n",
    "    # this should accept a 64 * m * n numpy array as query data, and returns the fen notation of the board.\n",
    "    def Predict(self, query_data):\n",
    "        grids = CNNClassifier.PreprocessImage(query_data)\n",
    "        y_pred = self.__model__.predict(grids).argmax(axis=1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    # predict by file name:\n",
    "    def PredictMultiple(self, file_names):\n",
    "        preds = []\n",
    "        truth = []\n",
    "        for f in file_names:\n",
    "            img = ReadImage(f, gray = False)\n",
    "            y_pred = LabelArrayToL(self.Predict(img))\n",
    "            y_true = FENtoL(GetCleanNameByPath(f))\n",
    "            preds.append(y_pred)\n",
    "            truth.append(y_true)\n",
    "        \n",
    "        all_pred = np.vstack(preds)\n",
    "        all_truth = np.vstack(truth)\n",
    "        return all_pred, all_truth\n",
    "    \n",
    "    \n",
    "    def LoadModel(self, name):\n",
    "        self.__model__.load_weights(name)\n",
    "    \n",
    "    def SaveModel(self, name):\n",
    "        os.makedirs(os.path.dirname(name), exist_ok = True)\n",
    "        self.__model__.save_weights(name)\n",
    "    \n",
    "    def PrintModel(self):\n",
    "        self.__model__.summary()\n",
    "    \n",
    "    def LoadMostRecentModel(self):\n",
    "        return self.LoadMostRecentModelFromDirectory(CNNClassifier.s_check_point_path)\n",
    "    \n",
    "    def LoadMostRecentModelFromDirectory(self, path):\n",
    "        try:\n",
    "            last_cp = tf.train.latest_checkpoint(path)\n",
    "            self.__model__.load_weights(last_cp)\n",
    "            print(\"Loaded checkpoint from \" + last_cp)\n",
    "            return True\n",
    "        except:\n",
    "            print(\"No checkpoint is loaded.\")\n",
    "            return False\n",
    "\n",
    "    def TestAccuracy(self, test_file_names):\n",
    "        num_files = len(test_file_names)\n",
    "\n",
    "        predict_result = self.__model__.predict(CNNClassifier.func_generator(test_file_names)).argmax(axis=1)\n",
    "        predict_result = predict_result.reshape(num_files, -1)\n",
    "        predicted_fen_arr = np.array([LtoFEN(LabelArrayToL(labels)) for labels in predict_result])\n",
    "        test_fens = np.array([GetCleanNameByPath(file_name) for file_name in test_file_names])\n",
    "\n",
    "        final_accuracy = (predicted_fen_arr == test_fens).astype(np.float).mean()\n",
    "        return final_accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def PreprocessImage(image):\n",
    "        image = transform.resize(image, (g_down_sampled_size, g_down_sampled_size), mode='constant')\n",
    "        \n",
    "        # 1st and 2nd dim is 8\n",
    "        grids = ImageToGrids(image, g_down_sampled_grid_size, g_down_sampled_grid_size)\n",
    "\n",
    "        # debug\n",
    "        #plt.imshow(grids[0][3])\n",
    "        #plt.show()\n",
    "\n",
    "        return grids.reshape(g_grid_row * g_grid_col, g_down_sampled_grid_size, g_down_sampled_grid_size, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test code for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 23, 23, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 21, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 19, 19, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 11552)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1478784   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                1677      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13)                0         \n",
      "=================================================================\n",
      "Total params: 1,499,853\n",
      "Trainable params: 1,499,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "cnn: loading model from ./saved_model/cnn_weights\n",
      "predicted: 1b1B1b2-2pK2q1-4p1rB-7k-8-8-3B4-3rb3\n",
      "Original:  1b1B1b2-2pK2q1-4p1rB-7k-8-8-3B4-3rb3\n",
      "confusion matrix:\n",
      " [[  92    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0  102    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0  157    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0   95    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0   43    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0  100    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0   82    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0   93    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  147    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   98    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0   35    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  100    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0 5256]]\n",
      "Balanced accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "if test_CNN_train:\n",
    "    cnn = CNNClassifier()\n",
    "    train_names = GetFileNamesInDir(g_train_dir)\n",
    "    cnn.Train(train_names)\n",
    "    cnn.SaveModel(cnn_model_file)\n",
    "\n",
    "if test_CNN_predict:\n",
    "    cnn = CNNClassifier()\n",
    "    cnn.PrintModel()\n",
    "    print(\"cnn: loading model from \" + cnn_model_file)\n",
    "    cnn.LoadModel(cnn_model_file)\n",
    "    predicted_label = cnn.Predict(ReadImage(a_random_file))\n",
    "    L = predicted_label\n",
    "    FEN = LtoFEN(LabelArrayToL(L))\n",
    "    print(\"predicted: \" + FEN)\n",
    "    print(\"Original:  \" + GetCleanNameByPath(a_random_file))\n",
    "\n",
    "    #test_file_names = GetFileNamesInDir(g_test_dir)[:1000]\n",
    "    #print(\"CNN: Testing accuracy for {} board images.\".format(len(test_file_names)))\n",
    "    #accuracy = cnn.TestAccuracy(test_file_names)\n",
    "    #print(\"CNN: Final accuracy: {}\".format(accuracy))\n",
    "    \n",
    "    y_pred, y_true = cnn.PredictMultiple(GetFileNamesInDir(g_test_dir)[:100])\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true.reshape((-1, 1)), y_pred.reshape((-1, 1)), labels = g_labels)\n",
    "    \n",
    "    print(\"confusion matrix:\\n\", conf_mat)\n",
    "    \n",
    "    print(\"Balanced accuracy: \", BalancedAccuracyFromConfusionMatrix(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test code for ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross validation for 3 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10-fold for CNN\n",
    "# random sampling rate of the each fold in 10-fold\n",
    "cnn_random_sampling_rate = 0.5\n",
    "\n",
    "if test_CNN_ten_fold:\n",
    "\n",
    "    train_file_names = GetFileNamesInDir(g_train_dir, extension = \"jpeg\")\n",
    "\n",
    "    cnn_tf = CNNClassifier()\n",
    "\n",
    "    confusion_matrices_cnn, accuracies_cnn, accuracies_balanced_cnn, train_time_cost_cnn, validation_time_cost_cnn = \\\n",
    "    ConfusionMatrix(cnn_tf, train_file_names, RandomFilter, sampling_rate = cnn_random_sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_CNN_ten_fold:\n",
    "    # dump the matrices for report.\n",
    "    os.makedirs(os.path.dirname(ten_fold_result_path), exist_ok = True)\n",
    "\n",
    "    np.save(ten_fold_result_path + \"confusion_matrices_cnn.npy\", confusion_matrices_cnn)\n",
    "    np.save(ten_fold_result_path + \"accuracies_cnn.npy\", accuracies_cnn)\n",
    "    np.save(ten_fold_result_path + \"accuracies_balanced_cnn.npy\", accuracies_balanced_cnn)\n",
    "    np.save(ten_fold_result_path + \"train_time_cost_cnn.npy\", train_time_cost_cnn)\n",
    "    np.save(ten_fold_result_path + \"validation_time_cost_cnn.npy\", validation_time_cost_cnn)\n",
    "\n",
    "    cnn_tf.SaveModel(cnn_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 10 fold results directly if needed.\n",
    "if test_CNN_ten_fold:\n",
    "    confusion_matrices_cnn = np.load(ten_fold_result_path + \"confusion_matrices_cnn.npy\")\n",
    "    accuracies_cnn = np.load(ten_fold_result_path + \"accuracies_cnn.npy\")\n",
    "    accuracies_balanced_cnn = np.load(ten_fold_result_path + \"accuracies_balanced_cnn.npy\")\n",
    "    train_time_cost_cnn = np.load(ten_fold_result_path + \"train_time_cost_cnn.npy\")\n",
    "    validation_time_cost_cnn = np.load(ten_fold_result_path + \"validation_time_cost_cnn.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: GUI: see GUI_with_Classifiers.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
